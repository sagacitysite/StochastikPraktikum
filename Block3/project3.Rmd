---
title: "Projektaufgaben Block 3"
author: "Carlo Michaelis, 573479; David Hinrichs, 572347; Lukas Ruff, 572521"
date: "10 Januar 2017"
documentclass: article
fontsize: 10pt
header-includes:
  - \usepackage{amsthm}
  - \usepackage{amssymb}
  - \usepackage{amsmath}
  - \DeclareMathOperator*{\argmin}{argmin}
  - \usepackage{MnSymbol}
  - \usepackage{bbm}
  - \usepackage{subfig}
  - \usepackage{theoremref}
  - \newtheorem{satz}{Satz}
  - \newcommand{\E}{\mathbb{E}}
  - \newcommand{\Var}{\text{Var}}
  - \newcommand{\tr}{\text{tr}}
output: 
  pdf_document:
    latex_engine: pdflatex
    keep_tex: true
    fig_caption: true
    number_sections: true
---

```{r setup, include=FALSE}
# Load libraries
library('Matrix')
library('wordcloud')
library('glmnet')
```

# SPAM vs. HAM: Naive Bayes

In dieser Aufgabe beschäftigen wir uns mit dem Spam vs. Ham Klassifizierungsproblem.

## Einlesen der Daten

```{r Read data}
# Read train/test data and dictionary
colFeatures <- c("docID", "wordID", "wordCount")

trainFeatures <- read.table(file = './data/train-features.txt', 
                            col.names = colFeatures)
testFeatures <- read.table(file = './data/test-features.txt',
                           col.names = colFeatures)

YTrain <- read.table(file = './data/train-labels.txt', col.names = "Y")
YTest <- read.table(file = './data/test-labels.txt', col.names = "Y")

# Read dictionary
colDict <- c("wordID", "word")
dict <- read.table(file = './data/dictionary.txt', col.names = colDict)
```

## Erzeugen der Featurematrizen

```{r Build feature matrices}
# Build (sparse) feature matrices
XTrain <- sparseMatrix(i = trainFeatures$docID, j = trainFeatures$wordID,
                       x = trainFeatures$wordCount)
XTest <- sparseMatrix(i = testFeatures$docID, j = testFeatures$wordID,
                      x = testFeatures$wordCount)
```

## Erzeuge Wahrscheinlichkeiten für den Naive-Bayes-Classifier

```{r Generate probabilites for Naive-Bayes-Classifier}
fnNBTrain <- function(X, Y) {
  # This function generates the probabilities for the Naive-Bayes-Classifier.
  # 
  # Args:
  #   X: Matrix of features
  #   Y: Vector of labels
  #   
  # Returns:
  #   A list containing the following elements:
  #     $phiSpam:   Probabilities for words occuring in SPAM message
  #     $phiHam:    Probabilities for words occuring in HAM message
  #     $gammaSpam: The relative frequency of SPAM messages
  #     $gammaHam:  The relative frequency of HAM messages
  
  nWords <- dim(X)[2]
  indSpam <- as.logical(Y)
  indHam <- !(indSpam)
  NBClass <- list()
  
  # Generate probabilities
  NBClass$phiSpam <- (colSums(X[indSpam, ]) + 1) / (sum(X[indSpam, ]) + nWords)
  NBClass$phiHam <- (colSums(X[indHam, ]) + 1) / (sum(X[indHam, ]) + nWords)
  
  # Get relative frequencies
  NBClass$gammaSpam <- sum(indSpam) / length(Y)
  NBClass$gammaHam <- sum(indHam) / length(Y)
  
  return(NBClass)
}

NBClass1 <- fnNBTrain(XTrain, YTrain[[1]])
```

Zähler und Nenner wurden derart angepasst, dass für den Fall, dass keine Trainingsdaten vorliegen, für die bedingten Verteilungen der Wörter in einem Dokument a priori diskrete Gleichverteilungen mit Wahrscheinlichkeiten $\frac{1}{|V|} = \frac{1}{2500}$ angenommen werden.

## Vorhersage auf den Testdaten

```{r Prediction}
fnNBPredict <- function(X, NBClass) {
  # This function predicts one of the two classes (SPAM or HAM) for some test
  # data X using a Naive-Bayes-Classifier trained by fnNBTrain.
  # 
  # Args:
  #   X:        Matrix of features
  #   NBClass:  A list returned by fnNBTrain which contains
  #               $phiSpam:   Probabilities for words occuring in SPAM message
  #               $phiHam:    Probabilities for words occuring in HAM message
  #               $gammaSpam: The relative frequency of SPAM messages
  #               $gammaHam:  The relative frequency of HAM messages
  # 
  # Returns:
  #   A vector with predictions for each example.
  
  # Predict labels (using logarithm)
  postSpam <- rowSums(t(t(X) * log(NBClass$phiSpam))) + log(NBClass$gammaSpam)
  postHam <- rowSums(t(t(X) * log(NBClass$phiHam))) + log(NBClass$gammaHam)
  pred <- (postSpam > postHam) * 1
  
  return(pred)
}

# Predict test labels
predTest1 <- fnNBPredict(XTest, NBClass1)

# Number of errors
nErr1 <- sum(YTest[[1]] != predTest1)
```

Insgesamt gibt es lediglich `r nErr1` falsche Klassifikationen, was einer Fehlerrate von `r round((nErr1 / length(YTest[[1]])) * 100, 2)`% entspricht. 

Schauen wir uns an, welche Wörter besonders gute Indikatoren für SPAM oder HAM sind. Dazu betrachten wir jeweils die 25 Wörter mit den größten $\Phi_{i|1}$ bzw.\ $\Phi_{i|0}$:

```{r Indicators 1, fig.cap = "Wordclouds for SPAM (left) and HAM (right) indicators."}
par(mfrow = c(1,2))

# Plot SPAM indicators
wordcloud(dict$word[rank(-NBClass1$phiSpam) <= 50], 
          NBClass1$phiSpam[rank(-NBClass1$phiSpam) <= 50],
          scale = c(2.5, 0.2))

# Plot HAM indicators
wordcloud(dict$word[rank(-NBClass1$phiHam) <= 50], 
          NBClass1$phiHam[rank(-NBClass1$phiHam) <= 50],
          scale = c(2.5, 0.2))
```

## Feature Engineering

Für die Klassifizierung von SPAM und HAM bedeutunglose Wörter sollten in SPAM- und HAM-Nachrichten relativ betrachtet ähnlich häufig vorkommen. Um Ähnlichkeit zu quantifizieren, können wir das Verhältnis von $\Phi_{i|1}$ zu $\Phi_{i|0}$ betrachten. Logarithmieren liefert eine bessere Skalierung.

```{r Removal of meaningless words}
logRelProbs <- log(NBClass1$phiSpam/NBClass1$phiHam)
hist(logRelProbs, breaks = 50)

# Remove some words
indMeaningful <- !((logRelProbs >= (-1.5)) & (logRelProbs <= 1.5))

# Test classification after removal of meaningless words
NBClass2 <- fnNBTrain(XTrain[, indMeaningful], YTrain[[1]])
predTest2 <- fnNBPredict(XTest[, indMeaningful], NBClass2)

# New classification error
nErr2 <- sum(YTest[[1]] != predTest2)
```

```{r Indicators 2, fig.cap = "Wordclouds for SPAM (left) and HAM (right) indicators after removal of meaningless words."}
wordsMeaningful <- dict$word[indMeaningful]

# New word clouds
par(mfrow = c(1,2))

# Plot SPAM indicators
wordcloud(wordsMeaningful[rank(-NBClass2$phiSpam) <= 50], 
          NBClass2$phiSpam[rank(-NBClass2$phiSpam) <= 50], 
          scale = c(2.5, 0.2))

# Plot HAM indicators
wordcloud(wordsMeaningful[rank(-NBClass2$phiHam) <= 50], 
          NBClass2$phiHam[rank(-NBClass2$phiHam) <= 50],
          scale = c(2.5, 0.2))
```

Durch die Entfernung bedeutungsloser Wörtern hat sich die Anzahl der Fehler von `r nErr1` auf `r nErr2` halbiert.

```{r Additional features}
# TODO: Implementation of additional features (e.g. number of words in doc)
# Maybe hints on: https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering
```

```{r Cross Validation}
# TODO: Implementation of m-fold cross validation
# Maybe plots for train/test error in dependence of training set size.
# Discussion on model generalization and overfitting.
```

```{r Raw Features (Bonus)}
# TODO: Extract the original emails from rawFeatures.zip
# Create the features in 1) and 5a) by hand

# Get the fileslists
hamTestDir = './data/rawFeatures/ham-test/'
hamTrainDir = './data/rawFeatures/ham-train/'
spamTestDir = './data/rawFeatures/spam-test/'
spamTrainDir = './data/rawFeatures/spam-train/'
hamTestFiles <- list.files(path = hamTestDir)
hamTrainFiles <- list.files(path = hamTrainDir)
spamTestFiles <- list.files(path = spamTestDir)
spamTrainFiles <- list.files(path = spamTrainDir)

# Get paths to individual files
hamTestFilesPath <- paste(hamTestDir, hamTestFiles, sep='')
hamTrainFilesPath <- paste(hamTrainDir, hamTrainFiles, sep='')
spamTestFilesPath <- paste(spamTestDir, spamTestFiles, sep='')
spamTrainFilesPath <- paste(spamTrainDir, spamTrainFiles, sep='')

fnReadStuff <- function(args) {
# function to read the raw data
# turn off warnings bc/ trailing whitespace leads to NA value and a warning in read.table
options(warn=-1)
# do stuff
#read.table(hamTestFilesPath, sep=' ')
options(warn=0)
}
```

# SPAM vs. HAM: Lineare Regression & Lasso

## Kleinste-Quadrate-Schätzer und Ridge-Regression-Schätzer

### a) Explizite Bestimmung des Ridge-Regression-Schätzers

Wir nehmen ein lineare Modell $Y = X\beta + \epsilon$ an, wobei $\epsilon \sim N(0,\sigma^2 I_n)$. Zunächst betrachten wir den Kleinste-Quadrate-Schätzer $\hat\beta$ und formulieren seine explizite Darstellung (siehe Folien):

$$
\hat\beta = \argmin_{b\in \mathbb{R}^p} \|Y-Xb\|^2 = (X^T X)^{-1} X^T Y
$$

Der Ridge-Regression-Schätzer, ist mit der $l^2$-Norm wie folgt definiert, wobei wir den zu minimierenden Ausdruck mit $Q$ bezechnen:

$$
\hat\beta^{RR} = \argmin_{b\in \mathbb{R}^p}  Q = \argmin_{b\in \mathbb{R}^p} \left( \|Y-Xb\|^2 + \lambda \|b\|^2_{l^2} \right)
$$

Durch Umstellung erhalten wir:

\begin{align*}
Q &= \|Y-Xb\|^2 + \lambda \|b\|^2_{l^2}\\
&= (Y-Xb)^T (Y-Xb) + \lambda b^T b\\
&= (Y^T-b^T X^T) (Y-Xb) + \lambda b^T b\\
&= Y^T Y - b^T X^T Y - Y^T X b + b^T X^T X b + \lambda b^T b\\
&= Y^T Y - 2 b^T X^T Y  + b^T X^T X b + \lambda b^T b
\end{align*}

Durch Ableitung erhalten wir den Ridge-Regression-Schätzer:

\begin{align*}
\frac{\partial Q}{\partial b} &= -2 X^T Y + 2 X^T X b + 2 \lambda b \overset{!}{=} 0\\
&\Leftrightarrow\; X^T Y = (X^T X + \lambda I_p) b\\
&\Leftrightarrow\; \hat\beta^{RR} = (X^T X + \lambda I_p)^{-1} X^T Y \\
\end{align*}

### b) Erwartungswert und Varianz

Wir beginnen mit dem Erwartungswert des Kleinste-Quadrate-Schätzers $\hat\beta$. Mit der Linearität des Erwartungswertes, sowie $\E[\epsilon] = 0$ aus der Annahme, gilt:

\begin{align*}
\E[\hat\beta] &= \E[(X^T X)^{-1} X^T Y]\\
&= (X^T X)^{-1} X^T \E[X \beta + \epsilon]\\
&= (X^T X)^{-1} X^T X \beta + (X^T X)^{-1} X^T \E[\epsilon] = \beta
\end{align*}

Bei der Bestimmung der Varianz von $\hat\beta$ nutzen wir erneut, dass der Ewartungswert der Fehler Null ist. Mit dem Verschiebungssatz gilt daher $\Var(\epsilon) = \E[\epsilon \epsilon^T] = \sigma^2 I_p$. Für die Varianz des Schätzers gilt:

\begin{align*}
\Var(\hat\beta) &= \E[(\hat\beta - \beta)(\hat\beta - \beta)^T]\\
&= \E\left[ \left( (X^T X)^{-1} X^T Y - \beta \right) \left( (X^T X)^{-1} X^T Y - \beta \right)^T \right]\\
&= \E\left[ \left( (X^T X)^{-1} X^T (X\beta + \epsilon) - \beta \right) \left( (X^T X)^{-1} X^T (X\beta + \epsilon) - \beta \right)^T \right]\\
&= \E\left[ \left( \beta + (X^T X)^{-1} X^T \epsilon - \beta \right) \left( \beta + (X^T X)^{-1} X^T \epsilon - \beta \right)^T \right]\\
&= \E\left[(X^T X)^{-1} X^T \epsilon \epsilon^T X (X^T X)^{-1} \right]\\
&= (X^T X)^{-1} X^T \E[\epsilon \epsilon^T] X (X^T X)^{-1}\\
&= (X^T X)^{-1} X^T \Var(\epsilon) X (X^T X)^{-1}\\
&= (X^T X)^{-1} X^T \sigma^2 I_n X (X^T X)^{-1}\\
&= \sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1}\\
&= \sigma^2 (X^T X)^{-1}\\
\end{align*}

Der Erwartungswert des Ridge-Resgression-Schäzers $\hat\beta^{RR}$ folgt analog zum obigen Vorgehen:

\begin{align*}
\E[\hat\beta^{RR}] &= \E[(X^T X + \lambda I_p)^{-1} X^T Y]\\
&= (X^T X + \lambda I_p)^{-1} X^T \E[X \beta + \epsilon]\\
&= (X^T X + \lambda I_p)^{-1} X^T X \beta + (X^T X + \lambda I_p)^{-1} X^T \E[\epsilon]\\
&= (X^T X + \lambda I_p)^{-1} X^T X \beta
\end{align*}

Für $\lambda \to 0$ gilt $\E[\hat\beta] = \E[\hat\beta^{RR}] = \beta$. Der Ridge-Regression-Schätzer entspricht dann dem Kleinste-Quadrate-Schätzer und wird entsprechend erwartungstreu. Für $\lambda \to \infty$ steigt der Bias des Ridge-Regression-Schätzers mit steigendem $\lambda$.

Die Varianz wird ebenfalls analog bestimmt:

\begin{align*}
\Var(\hat\beta^{RR}) &= \E[(\hat\beta^{RR} - \E[\hat\beta^{RR}])(\hat\beta^{RR} - \E[\hat\beta^{RR}])^T]\\
&= \E\left[ \left( (X^T X + \lambda I_p)^{-1} X^T Y - (X^T X + \lambda I_p)^{-1} X^T X \beta \right) \left( (X^T X + \lambda I_p)^{-1} X^T Y - (X^T X + \lambda I_p)^{-1} X^T X \beta \right)^T \right]\\
&= \E\left[ \left( (X^T X + \lambda I_p)^{-1} X^T \epsilon \right) \left( (X^T X + \lambda I_p)^{-1} X^T \epsilon \right)^T \right]\\
&= (X^T X + \lambda I_p)^{-1} X^T \E[\epsilon \epsilon^T] X (X^T X + \lambda I_p)^{-1}\\
&= \sigma^2 (X^T X + \lambda I_p)^{-1} X^T X (X^T X + \lambda I_p)^{-1}
\end{align*}

Auch hier gilt für $\lambda \to 0$, dass $\Var[\hat\beta] = \Var[\hat\beta^{RR}] = \sigma^2 (X^T X)^{-1}$. Für $\lambda \to \infty$ wird die Varianz des Ridge-Regression-Schätzers mit zunehmendem $\lambda$ immer kleiner.

### c) Mittlerer quadratischer Fehler

Der mittlere quadratische Fehler eines Schätzers $\hat\rho$ für den Parameter $\rho$ kann mittels Bias-Varianz-Zerlegung wie folgt formuliert werden:

$$
\E[\| \hat\rho - \rho \|^2] = \E[(\hat\rho - \rho)^T (\hat\rho - \rho)] = \left( \E[\hat\rho] - \rho \right)^T \left( \E[\hat\rho] - \rho \right) + \tr \left( \Var(\hat\rho) \right) = \text{Bias}(\hat\rho) + \text{Varianz}(\hat\rho)
$$

Die Bias-Varianz-Zerlegung lässt sich leicht zeigen indem mit einem $\E[\hat\rho]$ erweitert wird, d.h. $\E[(\hat\rho - \rho)^T (\hat\rho - \rho)] = \E[(\hat\rho - \E[\hat\rho] + \E[\hat\rho] - \rho)^T (\hat\rho - \E[\hat\rho] + \E[\hat\rho] - \rho)]$. Durch Ausmultiplikation folgt das Resultat.

Für den Kleinste-Quadrate-Schätzer $\hat\beta = (X^T X)^{-1} X^T Y$ ergibt sich damit:

$$
\E[\| \hat\beta - \beta \|^2] = (\beta - \beta)^T (\beta - \beta) + \tr \left( \Var(\hat\beta) \right) = \sigma^2\, \tr\left( (X^T X)^{-1} \right)
$$

Deutlich wird, dass der Schätzer keinen Bias aufweist, d.h. $\text{Bias}(\hat\beta) = 0$.

Für den Ridge-Resgression-Schätzer $\hat\beta^{RR} = (X^T X + \lambda I_p)^{-1} X^T Y$ gilt: 

\begin{align*}
\E[\| \hat\beta^{RR} - \beta \|^2] = &\left( (X^T X + \lambda I_p)^{-1} X^T X \beta - \beta \right)^T \left( (X^T X + \lambda I_p)^{-1} X^T X \beta - \beta \right)\\
&+ \sigma^2 \tr\left( (X^T X + \lambda I_p)^{-1} X^T X (X^T X + \lambda I_p)^{-1} \right)
\end{align*}

Der Ridge-Regression-Schätzer besitzt für $\lambda \ne 0$ einen Bias. Gleichzeitig wird die Varianz im Vergleich zum Kleinste-Quadrate-Schätzer kleiner. Damit "erkauft" man sich bei der Ridge-Regression eine geringere Varianz, der Preis dafür ist jedoch eine Verzerrung der Schätzung. Für ein gut gewähltes $\lambda$ kann dieser "Deal" vorteilhaft sein, z.B. wenn die Varianz des Kleinste-Quadrate-Schätzers so groß ist, dass keine vernünftige Aussage getroffen werden kann.

## Klassifikation mittels Lasso-Schätzer

### a) Implementierung des Klassifizierers

```{r Lasso}
fnRPredict <- function(X, Y, XTest, ...) {
  # This function predicts the labels for the test set using Ridge/Lasso regression
  # 
  # Args:
  #   X: Matrix of training features
  #   Y: Vector of training labels
  #   XTest: Vector of test features
  #   
  # Returns:
  #   Vector of predicted labels from test features
  
  # Fit spam data via lasso regularization and use binomial function
  fitGlm <- glmnet(X, Y, family = "binomial", ...)
  
  # Do cross validation to find optimal lambda
  cv <- cv.glmnet(X, Y)
  
  # Predict test data
  YPred <- predict(fitGlm, XTest, type="response", s=cv$lambda.min)
  
  # Classify test data
  YPred[YPred >= 0.5] <- 1
  YPred[YPred < 0.5] <- 0
  
  return(YPred)
}

## Lasso Regression
YPredLR <- fnRPredict(XTrain, YTrain[[1]], XTest, alpha = 1)

# Number of errors
nErrLR <- sum(YPredLR != YTest)

## Lasso Regression
YPredRR <- fnRPredict(XTrain, YTrain[[1]], XTest, alpha = 0)

# Number of errors
nErrRR <- sum(YPredRR != YTest)
```

Die Funktion `glmnet` ermöglicht es über den Parameter `alpha` eine Gewichtung zwischen $l^1$-Norm und $l^2$-Norm, also zwischen Lasso- und Ridge-Resgression, vorzunehmen. Im Extremfall kann damit eine pure Lasso- bzw. Ridge-Regression gerechent werden. Beide Fälle wurden durchgeführt. Im Fall der Lasso Regression erhalten wir `r nErrLR` falsche Predictions, bei einer Fehlerrate von `r round((nErrLR / length(YTest[[1]])) * 100, 2)`%. Im Fall der Ridge Regression kommt es zu `r nErrRR` falschen Predictions, was einer Fehlerrate von `r round((nErrRR / length(YTest[[1]])) * 100, 2)`% entspricht.

### b) Vergleich der Hauptergebnisse

Während es bei dem Naive Bayes Ansatz zu `r nErr1` Fehlern kam, kommt es bei Lasso zu `r nErrLR` Fehlern. Die Fehlerrate ist also etwas höher bei Lasso, aber immer noch recht niedrig. Zum Vergleich der Methoden, sollen einige Aspekte angesprochen werden:

  * Bei Lasso Regression lässt sich die Modellierung zusätzlicher Features deutlich leichter gestalten als beim Bayes-Ansatz. Hier müssen lediglich weitere Variablen hinzugefügt werden, während im Bayes-Ansatz eine komplziertere Modellierung erfolgen muss.
  * Beim Regression-Ansatz erfolgt durch die Verwendung des verallgemeinerten linearen Modells eine Prädiktion der Wahrscheinlichkeit für eine Klasse, es werden nicht die Klassen direkt geschätzt. Dieser Aspekt ermöglicht die Wahl einer gut interpretierbaren Threshold. Beispielsweise könnte definiert werden, dass nur solche E-Mails als Spam klassifiziert werden, die mit 80 prozentiger Sicherheit auch tatsächlich Spam sind. Beim Bayes-Ansatz lässt sich ähnliches durch einen Koeffizienten beim Vergleich der Posteriors erreichen, dieser lässt sich jedoch nicht so anschaulich interpretieren, wie es bei der Lasso-Regression der Fall ist.
  * Beim Bayes-Ansatz ist es durch die Verwendung eines Priors möglich die Ergebnisse optimieren zu können, was beim Regressions-Ansatz nicht möglich ist.

# PCA & Eigenfaces

## Aufgaben

```{r PCA}
# TODO: ...
#
#
```


<!--Footnotes-->