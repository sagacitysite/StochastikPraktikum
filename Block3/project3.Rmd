---
title: "Projektaufgaben Block 3"
author: "Carlo Michaelis, 573479; David Hinrichs, 572347; Lukas Ruff, 572521"
date: "10 Januar 2017"
documentclass: article
fontsize: 10pt
header-includes:
  - \usepackage{amsthm}
  - \usepackage{amssymb}
  - \usepackage{amsmath}
  - \DeclareMathOperator*{\argmin}{argmin}
  - \usepackage{MnSymbol}
  - \usepackage{bbm}
  - \usepackage{subfig}
  - \usepackage{theoremref}
  - \newtheorem{satz}{Satz}
output: 
  pdf_document:
    latex_engine: pdflatex
    keep_tex: true
    fig_caption: true
    number_sections: true
---

```{r setup, include=FALSE}
# Load libraries
library('Matrix')
```

# SPAM vs. HAM: Naive Bayes

In dieser Aufgabe beschäftigen wir uns mit dem Spam vs. Ham Klassifizierungsproblem.

## Einlesen der Daten

```{r Read data}
# Read train/test data and dictionary
colFeatures <- c("docID", "wordID", "wordCount")

trainFeatures <- read.table(file = './data/train-features.txt', 
                            col.names = colFeatures)
testFeatures <- read.table(file = './data/test-features.txt',
                           col.names = colFeatures)

YTrain <- read.table(file = './data/train-labels.txt', col.names = "Y")
YTest <- read.table(file = './data/test-labels.txt', col.names = "Y")

# Read dictionary
colDict <- c("wordID", "word")
dict <- read.table(file = './data/dictionary.txt', col.names = colDict)
nWords <- dim(dict)[1]
```

## Erzeugen der Featurematrizen

```{r Build feature matrices}
# Build (sparse) feature matrices
XTrain <- sparseMatrix(i = trainFeatures$docID, j = trainFeatures$wordID,
                       x = trainFeatures$wordCount)
XTest <- sparseMatrix(i = testFeatures$docID, j = testFeatures$wordID,
                      x = testFeatures$wordCount)
```

## Erzeuge Wahrscheinlichkeiten für den Naive-Bayes-Classifier

```{r Generate probabilites for Naive-Bayes-Classifier}
# Generate probabilites
indSpam <- as.logical(YTrain[[1]])
indHam <- !(YTrain[[1]])
 
phiSpam <- (colSums(XTrain[indSpam, ]) + 1) / (sum(XTrain[indSpam, ]) + nWords)
phiHam <- (colSums(XTrain[indHam, ]) + 1) / (sum(XTrain[indHam, ]) + nWords)
```

Zähler und Nenner wurden derart angepasst, dass für den Fall, dass keine Trainingsdaten vorliegen, für die bedingten Verteilungen der Wörter in einem Dokument a priori diskrete Gleichverteilungen mit Wahrscheinlichkeiten $\frac{1}{|V|} = \frac{1}{2500}$ angenommen werden.

## Vorhersage auf den Testdaten

```{r Prediction}
# Prior probabilities of message being Spam or Ham are equal:
sum(indSpam) == sum(indHam)

# Predict test labels (using logarithm)
postSpam <- rowSums(t(t(XTest) * log(phiSpam)))
postHam <- rowSums(t(t(XTest) * log(phiHam)))
predTest <- (postSpam > postHam) * 1

# Number of errors
sum(YTest[[1]] != predTest)
```

```{r Indicators}
# SPAM indicators
dict$word[rank(-phiSpam) <= 25]

# HAM indicators
dict$word[rank(-phiHam) <= 25]
```




<!--Footnotes-->