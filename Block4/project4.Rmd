---
title: "Projektaufgaben Block 4"
author: "Carlo Michaelis, 573479; David Hinrichs, 572347; Lukas Ruff, 572521"
date: "31 Januar 2017"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: yes
  html_document: default
header-includes:
- \usepackage{amsthm}
- \usepackage{amssymb}
- \usepackage{amsmath}
- \DeclareMathOperator*{\argmin}{argmin}
- \usepackage{MnSymbol}
- \usepackage{bbm}
- \usepackage{subfig}
- \usepackage{theoremref}
- \newtheorem{satz}{Satz}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\Var}{\text{Var}}
- \newcommand{\tr}{\text{tr}}
fontsize: 10pt
documentclass: article
---

```{r setup, include=FALSE}
# Load libraries
library('ggplot2')
library(xtable)
```

# Markovketten zur Modellierung von Krebswachstum

Lade die Übergangsmatrix $P$ aus der Datei `cancer.csv`:

```{r Read data}
# Read transition matrix
P <- as.matrix(read.csv("data/lungcancer.csv"))
nStates <- dim(P)[1]

# Sanity check
apply(P, 1, sum)
```

Um die mittlere Anzahl von Schritten $\E[\tau_x]$ für Position $x \in \{1,\ldots,50\}$ zu approximieren, implementieren wir zwei Methoden. In Methode 1 nutzen wir aus, dass 
\[
  \mathbb{P}(\tau_x = k) = \sum_{i,j \not = x} \tilde{P}_{ij}^{k-1} P_{jx} \mu(i)
\]
gilt, um mittels
\[
  \E[\tau_x] =\sum_{k=0}^\infty k \mathbb{P}(\tau_x = k)
\]
die mittlere Anzahl von Schritten zu approximieren, wobei $\mu$ die Anfangsverteilung und $P$ die Übergangsmatrix ist. $(\tilde{P}_{ij})$ bezeichnet die Übergangsmatrix $P$, wenn die $x$-te Spalte und die $x$-te Zeile von $P$ durch Nullen ersetzt werden, mit $\tilde{P}^{k-1} = \tilde{P}^{k-2}\tilde{P}$.

```{r Mean number of steps for cancer to reach position (Method 1)}
# Method 1

fnMCMeanSteps1 <- function(x, mu, P, tailProb = 10^-3){
  # This function computes the mean number of steps needed to reach state x for
  # a Markov chain defined with initial distribution mu and transition matrix P.
  # 
  # Args:
  #   x:        Target state
  #   mu:       Initial distribution
  #   P:        Transition matrix
  #   tailProb: Tail probability to control approximation
  #   
  # Returns:
  #   Expected number of steps to reach state x for the first time
  
  # Step 0
  cumProbs <- mu[x]
  mean <- mu[x] * 0  # symbolic initialization
  
  # Step 1
  tempProb <- sum(mu[-x] * P[-x, x])
  cumProbs <- cumProbs + tempProb
  mean <- mean + tempProb * 1
  
  # Further steps
  PTilde <- diag(length(mu) - 1)  # initialize PTilde
  k <- 2
  
  while (1-cumProbs >= tailProb) {
    PTilde <- PTilde %*% P[-x, -x]  # Update PTilde
    tempProb <- mu[-x] %*% PTilde %*% P[-x, x]  # Get prob. of hitting time k
    cumProbs <- cumProbs + tempProb  # cumulate probability mass
    mean <- mean + tempProb * k  # update mean
    k <- k+1
  }
  
  # Return mean
  return(as.double(mean))
}
```

In Methode 2 simulieren wir 10.000 Markovketten und approximieren $\E[\tau_x]$ durch Mittelwertbildung. Dabei wählen wir einen maximalen Zeithorizont $T$ und berücksichtigen nur Pfade bei denen die Position $x$ auch erreicht wird.

```{r Mean number of steps for cancer to reach position (Method 2)}
# Method 2

fnMCMeanSteps2 <- function(x, mu, P, T = 1000){
  # This function computes the mean number of steps needed to reach state x for
  # a Markov chain defined with initial distribution mu and transition matrix P.
  # This method approximates the mean using 10'000 Markov chain simulations with
  # a maximum of T steps per simulation.
  # 
  # Args:
  #   x:  Target state
  #   mu: Initial distribution
  #   P:  Transition matrix
  #   T:  Maximum length of Markov chain
  #   
  # Returns:
  #   Expected number of steps to reach state x for the first time
  
  # Number of states and simulations
  nStates <- length(mu)
  nSim <- 10^4
  
  # Initialize vector to hold number of steps taken to reach state x for the
  # first time for each simulated markov chain
  vSteps <- vector(mode = "integer", length = nSim)
  
  # Start simulation of nSim markov chains with a maximum number of T steps
  for (n in 1:nSim){
    # Sample first state
    state <- sample(1:nStates, 1, prob = mu)
    
    for (t in 2:T+1){
      if (state == x){
        # break if state is x
        vSteps[n] <- t-1  # store number of steps
        break
      } else if (t == T+1) {
        # catch if state x has not been reached in T steps with NA
        vSteps[n] <- NA
        break
      }
      
      # sample next state depending on current state
      state <- sample(1:nStates, 1, prob = P[state, ])
    }
  }

  # Return mean
  return(mean(vSteps, na.rm = TRUE))
}
```

```{r Test Methods, cache = TRUE}
# Define initial distribution (always begin in state 23 (Lung))
mu <- rep(0, nStates)
mu[23] <- 1

# Expected number of steps to reach certain positions (Bladder, Brain, Heart)
 
# Method 1
startTime <- Sys.time()
meanStepsBladder1 <- fnMCMeanSteps1(5, mu, P, tailProb = 10^-5)
meanStepsBrain1 <- fnMCMeanSteps1(7, mu, P, tailProb = 10^-5)
meanStepsHeart1 <- fnMCMeanSteps1(17, mu, P, tailProb = 10^-5)
endTime  <- Sys.time()
timeMethod1 <- endTime - startTime

# Method 2
set.seed(42)
startTime <- Sys.time()
meanStepsBladder2 <- fnMCMeanSteps2(5, mu, P, T = 10000)
meanStepsBrain2 <- fnMCMeanSteps2(7, mu, P, T = 10000)
meanStepsHeart2 <- fnMCMeanSteps2(17, mu, P, T = 10000)
endTime  <- Sys.time()
timeMethod2 <- endTime - startTime
```

Methode 1 benötigte **`r round(as.double(timeMethod1, units = "secs"), 1)` Sekunden**, wohingegen Methode 2  **`r round(as.double(timeMethod2, units = "mins"), 1)` Minuten** für die Approximation benötigte. Die Ergebnisse für Blase, Gehirn und Herz sind in folgender Tabelle zusammengefasst:

```{r Results Table, echo = FALSE, results = "asis"}
print(xtable(matrix(c(meanStepsBladder1, meanStepsBrain1, meanStepsHeart1,
                      meanStepsBladder2, meanStepsBrain2, meanStepsHeart2),
                    ncol = 2, dimnames = list(
                      c("Bladder", "Brain", "Heart"),
                      c("Method 1", "Method 2"))
                    ),
             caption = "Expected number of steps to reach position in body."),
      comment = FALSE)
```


# Poisson-Prozess

```{r Read Data}
N <- as.matrix(read.csv("data/traffic.csv", header = TRUE))

# Get vector of 96 data points
Nt <- as.vector(t(N[,2:5]))

# Calculate lambda tilde and lambda as function of t
lambdaTildeT <- Nt
lambdaT <- 1.2 * lambdaTildeT
```

```{r Simulate Poisson Traffic}
fnSimulatePoisson <- function(lambda, n) {
  # This function simulates poisson values based on vector of parameters lambda
  # 
  # Args:
  #   lambda:   Vector of parameters for poisson distribution
  #   n:        Number of simulations
  #   
  # Returns:
  #   Simulated vectors as matrix, where every row is a simulation of one day
  #   therefore the matrix as n rows.
  
  # Get random poisson distributed values for n days
  vSimulated <- rpois(length(lambda)*n, lambda)
  
  # Split data into matrix of form: mSplitted[day, quarter-hour]
  # where day goes from 1 to n and quater-hour from 1 to 96
  mSplitted <- t(matrix(vSimulated, nrow=length(lambda), ncol=n))
  
  return(mSplitted)
}

# Define number of simulations
nSimulations = 10000

# Run poisson simulation, based on parameter values from traffic.csv
mSimulation <- fnSimulatePoisson(lambdaT, nSimulations)
```

```{r Evaluate Results}
## Task 1
meanCarsPerDay <- mean(rowSums(mSimulation))

## Task 2
thousandsCar <- NULL
for(i in 1:nSimulations) {
  # Calculate cumulative sums per day (per simulation)
  cumsumDay <- cumsum(mSimulation[i,])
  
  # It's not clear if floor should ne applies only for mean, or also here
  # However, in the end the results seems to be the same
  thousandsCar[i] <- floor(max(which(cumsumDay <= 1000))/4)
}

meanThousandsCar <- floor(mean(thousandsCar))

## Task 3
# Time between 1:30 and 1:45 is at position 7
meanCarsAtNightQuarter <- mean(as.vector(mSimulation[,7]))
# The result is 0, which we can easily explain as lambda is 0 in this timeslot
# More input data, to have a more precise model, would possibly change this

## Task 4/5
maxTrafficPeriodIndices <- NULL
maxTrafficPeriodValues <- NULL
for(i in 1:nSimulations) {
  # Calculate cumulative sums per day (per simulation)
  cumsumDay <- cumsum(mSimulation[i,])
  
  # Initialize new vector
  sumHourlyPeriod <- NULL
  
  # Run from 0.0 to 23.0
  for(j in 1:(length(cumsumDay)-4)) {
    sumHourlyPeriod[j] <- cumsumDay[j+4] - cumsumDay[j]
  }
  
  # Get maximum position and store in vector
  maxTrafficPeriodIndices[i] <- which.max(sumHourlyPeriod)
  maxTrafficPeriodValues[i] <- max(sumHourlyPeriod)
}

par(mfrow = c(1,2))

hist(maxTrafficPeriodIndices,
     breaks = max(maxTrafficPeriodIndices)-min(maxTrafficPeriodIndices),
     main = paste("4) Histogramm bzgl. ", nSimulations, "Simulationen"),
     xlab = "Einstündige Periode mit maximalem Traffic", ylab = "Häufigkeit")
grid()
# where eg. 30 is the time period between 07:15 - 08:15,
# 31 is 07:30 - 08:30, and so on

hist(maxTrafficPeriodValues,
     main = paste("5) Histogramm bzgl. ", nSimulations, "Simulationen"),
     xlab = paste("Anzahl der vorbeifahrende Fahrzeuge in einstündiger Periode",
                  "mit maximalem Traffic"),
     ylab = "Häufigkeit")
grid()
```

# Der betrunkene Vogel

In dieser Aufgabe beschäftigen wir uns mit dem Unterschied in State Recurrence zwischen zwei- und dreidimensionalen Random Walks.
Anders gesagt: Ein betrunkener Mensch (d=2) findet meist nach Hause, ein betrunkener Vogel (d=3) aber vielleicht nicht.

Dazu betrachten wir eine brownsche Bewegung $B_t$, die vom Punkt x ausgeht...

```{r illustrate drunk bird and human}
# check if library is there
require('ggplot2')

fnRandomWalk <- function(d, x, N, R) {
  # This function generates a random walk of dimension d
  #
  #
  dfWalk = data.frame(matrix(nrow=N, ncol=d+1))
  
  n=1
  pos=x
  dfWalk[1,1:d] = pos
  dist = sqrt(sum(pos*pos))
  dfWalk[1,d+1] = dist
  
  while(n<N && R<dist) {
    # Thi is the loop handling the random walk
    rstep <- c(round(rnorm(d),2))
    pos = pos+rstep
    dist <- sqrt(sum(pos*pos))
    dfWalk[n+1,1:d] = pos+rstep
    dfWalk[n+1,d+1] = dist
    n = n+1 
  }
  # na.omit ist necessary to handle random walks exiting after n<N (made it home)
  return(na.omit(dfWalk))
}

set.seed(2567)
humanWalk <- fnRandomWalk(2, c(10,10), 1000, 3)
birdWalk <- fnRandomWalk(3, c(10,10,10), 1000, 3)
```

## Plots für Menschen und Vogel

Mensch und Vogel besuchen die gleiche Kneipe (die des Vogels liegt auf dem Dach der Bar), und torkeln nach getanem Werk nach Hause:

```{r human walk, echo=FALSE}
# Plotting human random walk
start <- data.frame(X=10, Y=10, Z=10)
goal <- data.frame(X=0, Y=0, Z=10)
p1 <- ggplot(humanWalk, aes(x=X1, y=X2, color=X3)) +
    geom_path() +
    geom_point(data=start,aes(X,Y),colour='red',size=4) +
    geom_point(data=goal,aes(X,Y),colour='green',size=4)
p1
```

Der Mensch muss sich nur in $R²$ bewegen...

```{r bird walk, echo=FALSE}
# Plotting bird random walk
birdWalk <- fnRandomWalk(3, c(10,10,10), 1000, 3)
p2 <- ggplot(birdWalk, aes(x=X1, y=X2, color=X4)) +
    geom_path() +
    geom_point(data=start,aes(X,Y),colour='red',size=4) +
    geom_point(data=goal,aes(X,Y),colour='green',size=4)
p2

```

während der Vogel in $R³$ Probleme bekommt.


## Monte Carlo

Jetzt simulieren wir den random walk für d=2 und d=3 mit einer Monte Carlo Simulation
```{r Simulate random walks}

a <- proc.time()
set.seed(4579)
walks <- 1000
steps <- 10000
length <- 10000
startpos <- c(10,10,10)
R <- 3

randValsX <- matrix(rnorm(walks*steps,0,length/steps),nrow=walks,ncol=steps)
randValsY <- matrix(rnorm(walks*steps,0,length/steps),nrow=walks,ncol=steps)
randValsZ <- matrix(rnorm(walks*steps,0,length/steps),nrow=walks,ncol=steps)
randValsX[,1] <- startpos[1]
randValsY[,1] <- startpos[2]
randValsZ[,1] <- startpos[3]
randValsX <- t(apply(randValsX,1,cumsum))
randValsY <- t(apply(randValsY,1,cumsum))
randValsZ <- t(apply(randValsZ,1,cumsum))

WalksH <- sqrt((randValsX^2)+(randValsY^2)) < R
WalksB <- sqrt(((randValsX^2)+(randValsY^2)+(randValsZ^2))) < R

fnWalkIn <- function(slice) {
  len <- length(slice)
  first <- match(1,slice)
  if(is.finite(first)) {
    slice[first:len] <- 1
  }
  return(slice)
}

WalksH <- t(apply(WalksH,1,fnWalkIn))
WalksB <- t(apply(WalksB,1,fnWalkIn))

b <- proc.time()
b-a
WalksInH <- apply(WalksH,2,sum)
WalksInB <- apply(WalksB,2,sum)

####################################
```

```{r plot MC, echo=FALSE}
H <- qplot(x=1:length(WalksInH), y=WalksInH,geom="line", xlab="", ylab="",
      main="Walks 'home' after x steps")
B <- qplot(x=1:length(WalksInB), y=WalksInB,geom="line", xlab="", ylab="",
      main="Walks 'home' after x steps")
H
B

```

Die y-Achse zeigt den %-Anteil an random walks, die nach Hause führen. Die x-Achse zeigt die Anzahl der gelaufenen Schritte.

# Geometrische Brownsche Bewegung und Option Pricing

## Verhalten der geometrischen Brownschen Bewegung

```{r}

```

## Arbitrage-freier Preis einer europäischen Call-Option

Black und Scholes (1973) modellieren $X_t$, den Preis eines Wertpapiers zum Zeitpunkt $t$, durch eine geometrische Brownsche Bewegung. Der arbitrage-freie Preis $C$ einer europäischen Call-Option mit Ausübungszeit $T$ und Ausübungspreis $K$ ist dann gegeben durch
\[
  C = e^{-rT}\E^*[(X_T - K)_+],
\]
wobei $r > 0$ die risikolose Zinsrate und $X_T$ unter dem Maß $\mathbb{P}^*$ eine geometrische Brownsche Bewegung mit Parametern $\mu = r$ und $\sigma$ ist. \\

Im Folgenden seien $T=1$, $r = 0.04$, $\sigma = 0.2$ und $X_0 = K = 100$. \\
```{r Set parameter}
# Set parameter of geometric brownian motion and call option
TCall <- 1
r <- 0.04
sigma <- 0.2
X0 <- 100
K <- 100
C <- list()
```

Nach der expliziten Formel, die von Black und Scholes 1973 gezeigt wurde, folgt für den arbitrage-freien Preis $C$:

```{r Black and Scholes solution}
# Black-Scholes
d1 <- 1/(sigma*sqrt(TCall)) * (log(X0/K)+(r+(0.5*sigma^2))*TCall)
d2 <- d1-sigma*sqrt(TCall)
C$BlackScholes <- X0*pnorm(d1) - K*exp(-r*TCall)*pnorm(d2)
```

Als zweite Methode wollen wir $C$ durch Simulation von $(X_t)_{0\leq t\leq1}$ mittels der exakten Lösung der geometrischen Brownschen Bewegung, d.h.\ 
\[
  X_t = X_0 \exp\left(\left(\mu-\frac{\sigma^2}{2}\right)t + \sigma B_t\right),
\]
mit Hilfe von Monte-Carlo-Simulationen berechnen. Aufgrund der exakten Lösung für $X_t$ müssen wir lediglich die Brownsche Bewegung $B_t$ simulieren.

```{r MC Simulation with exact formula}
fnBrownianMotion <- function(nSim, nSteps, tMax){
  # This function simulates nSim brownian motions with nSteps of length 
  # (tMax/nSteps). That is, the browian motions are of overall length of tMax.
  # 
  # Args:
  #   nSim:   Number of simulations/walks
  #   nSteps: Number of steps in each walk
  #   tMax:   Overall length of each walk
  #   
  # Returns:
  #   A matrix with nSim rows and nSteps columns, i.e. each row is the simulation
  #   of one brownian motion.
  matSteps <- matrix(rnorm(nSim*nSteps, 0, tMax/nSteps), 
                     nrow = nSim, ncol = nSteps)
  return(t(apply(matSteps, 1, cumsum)))
}

# Simulate brownian motions
nSim <- 10000
nSteps <- 1000
matBM <- fnBrownianMotion(nSim, nSteps, TCall)

# Compute price processes using exact solution of SDE
times <- seq(TCall/nSteps, TCall, 1/nSteps)
matX <- t(X0 * exp((mu-0.5*sigma^2)*times + sigma*t(matBM)))

# Get MC approximation of arbitrage-free price C
payouts <- matX[, nSteps] - K
payouts[payouts <= 0] <- 0

C$MC <- exp(-r) * mean(payouts)
```


```{r geometric brownian motion MC}
############## Monte Carlo
walks <- 1000
steps <- 100
TCalls <- c(seq(0.01,1,1/steps))

randValsBrown <- matrix(rnorm(walks*steps,0,TCalls/steps),nrow=walks,ncol=steps)
randValsBrown <- t(apply(randValsBrown,1,cumsum))
mu <- r
X_t <- X0*exp((mu-0.5*sigma^2)*TCalls+sigma*randValsBrown)

expVals <- (X_t - K)
######
rep.row<-function(x,n){
   matrix(rep(x,each=n),nrow=n)
}
TCalls <- rep.row(TCalls,walks)
######
smth <- exp(-r*TCalls)/(walks)*expVals
smth[smth<0] <- 0
C$MC <- apply(smth/walks,2,sum)

## Euler
#### confusing formula on slides
X_k <- X_k_1 + mu*((k-1)*T)




C$Euler <- NULL
## plotting
plot(C$MC)
abline(h=C$BlackScholes,col='red')

```


Die Black-Scholes Formel [Formel] ergibt...

Die MC Simulation ...

Das Euler-Schema...


<!--Footnotes-->