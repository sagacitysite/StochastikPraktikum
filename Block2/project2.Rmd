---
title: "Projektaufgaben Block 2"
author: "Carlo Michaelis, 573479; David Hinrichs, 572347; Lukas Ruff, 572521"
date: "06 Dezember 2016"
output:
  html_document: default
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: yes
header-includes:
- \usepackage{amsthm}
- \usepackage{amssymb}
- \usepackage{MnSymbol}
- \usepackage{bbm}
- \usepackage{subfig}
- \usepackage{theoremref}
- \newtheorem{satz}{Satz}
fontsize: 10pt
documentclass: article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Nichtparametrisches Testen

## Zwillingsstudie

Um zu testen, ob der Kindergartenbesuch einen signifikanten Einfluss auf die sozialen Fähigkeiten eines Kindes hat, führen wir einen zweiseitigen/einseitigen $t$-Test und einen zweiseitigen/einseitigen Wilcoxon-Vorzeichen-Test, jeweils zum Signifikanzniveau $\alpha = 0.05$, durch.

```{r Twins}
# Enter data
x <- c(82,69,73,43,58,56,76,65)
y <- c(63,42,74,37,51,43,80,62)
testType <- c('two-sided', 'one-sided')
pValueT <- c(0,0); pValueWilcox <- c(0,0)

# t-test
pValueT[1] <- t.test(x, y, alternative = "two.sided", mu = 0, conf.level = 0.95, paired = TRUE)$p.value
pValueT[2] <- t.test(x, y, alternative = "greater", mu = 0, conf.level = 0.95, paired = TRUE)$p.value

# Wilcoxon signed rank test
pValueWilcox[1] <- wilcox.test(x, y, alternative = "two.sided", mu = 0, conf.level = 0.95, 
            paired = TRUE, conf.int = TRUE)$p.value
pValueWilcox[2] <- wilcox.test(x, y, alternative = "greater", mu = 0, conf.level = 0.95, 
            paired = TRUE, conf.int = TRUE)$p.value


knitr::kable(data.frame(testType, pValueT, pValueWilcox), caption = 'p-Werte')

```

#### Zweiseitige Tests
Wir können sehen, dass der $t$-Test die Nullhypothese ablehnt ($p = 0.04895 < \alpha$) und somit einen signifikanten Einfluss feststellt.
Der Wilcoxon-Vorzeichen-Test hingegen ($p = `r pValueWilcox[1]` > \alpha$) lehnt die Nullhypothese nicht zum gegebenen Signifikanzniveau nicht ab.


#### Einseitige Tests
Im Fall zweiseitger Tests unter der gleichen Nullhypothese, und der Alternative dass die sozialen Fähigkeiten signifikant höher sind, lehnen nun beide Tests die Nullhypothese ab.
Der Grund liegt darin, dass hier die $p$-Werte nur eine Ablehnregion in die Richtung "größer" abdecken müssen und aufgrund der Symmetrie der Tests deshalb nur halb so groß sind wie bei den zweiseitigen Tests. Beide Tests indizieren nun einen signifikanten Anstieg der sozialen Fähigkeiten durch den Kindergartenbesuch.


#### Testannahmen
Schließlich lohnt es, einen Blick auf die Testannahmen zu richten. Während der Wilcoxon-Test lediglich eine symmetrische Verteilung voraussetzt, nimmt der t-Test konkret die Normalverteilung, $X_i - Y_i \sim N(0,\sigma^2)$, der Daten an. Da diese stärkere Annahme ihm auch zu seiner größeren Power verhilft, sollte man ihre Validität überprüfen, hier geschieht dies mit Hilfe eines QQ-Plot:
```{r Normalverteilungsannahme, echo=FALSE}
qqnorm(((x-y)-mean(x-y))/sd(x-y)); qqline(((x-y)-mean(x-y))/sd(x-y))
skewness = mean(x-y) - median(x-y)
```
Wie besonders am rechten Tail zu erkennen, kann die Normalverteilungsannahme durchaus in Frage gestellt werden.

Doch auch die Annahme symmetrischer Daten ist angesichts der "Schiefe" (`r skewness`) der Daten anzuzweifeln, aber sie ist deutlich schwächer als die zusätzliche Annahme einer (bis auf Paramter) exakten Verteilung.

Das Hauptproblem dieser Stichprobe ist jedoch ihre geringe Größe von N = 8, jedwede Art von Statistik besitzt nur sehr geringe Aussagekraft.




## $t$-Test vs.\ Wilcoxon-Vorzeichen-Test

Als nächstes schätzen wir die Power beider Tests in verschiedenen Settings ab. Dafür wird zunächst der wahre Wert $\theta \in [0, 0.5, 1] $ mit Fehlern verrauscht, die der Normal-/Cauchy-/ und Gleichverteilung folgen.

Blablabla

```{r fnTestPowerMC}
fnTestPowerMC <- function(fnError, n = 30, alpha = 0.05, nSim = 10^4, ...) {
  # This function estimates the probability of rejecting the null hypothesis of a
  # t-test and a Wilcoxon signed rank test using Monte Carlo simulations of iid
  # random variables X_i = \theta + \epsilon_i.
  # 
  # Args:
  #   fnError:  Function which generates random samples from a symmetric error 
  #             distribution \epsilon
  #   n:        Number of random samples used in tests
  #   alpha:    Significance level used in tests
  #   nSim:     Number of MC simulations of size n
  #   ...:      Further arguments to be passed to fnError
  #   
  # Returns:
  #   A list containing the following elements:
  #     $TProb:       MC estimation of rejection probability for the t-test
  #     $WilcoxProb:  MC estimation of rejection probability for the Wilcoxon
  #                   signed rank test
  
  # Perform MC simulation
  matX <- matrix(fnError(n*nSim, ...), ncol = n)
  
  # Define sub-functions which only return p-values from the two tests
  fnPvalT <- function(x) {
    return(t.test(x)$p.value)
  }
  fnPvalWilcox <- function(x) {
    return(wilcox.test(x)$p.value)
  }
  
  # Perform nSim number of tests with sample size n for each of the two tests
  vecPvalT <- apply(matX, 1, fnPvalT)
  vecPvalWilcox <- apply(matX, 1, fnPvalWilcox)
  
  # Compute and return estimations of rejection probabilities
  result <- list()
  result$TProb <- mean(vecPvalT < alpha)
  result$WilcoxProb <- mean(vecPvalWilcox < alpha)
  return(result)
}
```

```{r Perform MC simulations, cache = TRUE}
# Set seed
set.seed(432)

# Normal errors
normalMean = c(0, 0.5, 1)
normalSD = c(1, 1, 1)
normalPowerT <- list()
normalPowerW <- list()

for (i in 1:3) {
  res <- fnTestPowerMC(fnError = rnorm, nSim = 10^4,
                       mean = normalMean[i], sd=normalSD[i])
  normalPowerT[i] <- res[1]
  normalPowerW[i] <- res[2]
}
# Cauchy errors (t-distribution with df = 1)
cauchyPowerT <- list()
cauchyPowerW <- list()
cauchyLoc = c(0, 0.5, 1)
cauchyScale = c(1,1,1)
for (i in 1:3) {
  res <- fnTestPowerMC(fnError = rcauchy, nSim = 10^4,
                       location = cauchyLoc[i], scale=cauchyScale[i])
  cauchyPowerT[i] <- res[1]
  cauchyPowerW[i] <- res[2]
}

# Uniform errors
uniMin <- c(-1, -0.5, 0)
uniMax <- c(1, 1.5, 2)
uniPowerT <- list()
uniPowerW <- list()

for (i in 1:3) {
  res <- fnTestPowerMC(fnError = runif, nSim = 10^4,
                       min = uniMin[i], max = uniMax[i])
  uniPowerT[i] <- res[1]
  uniPowerW[i] <- res[2]
}

```

```{r echo=FALSE, results='asis'}

dfNormal <- data.frame(normalMean, normalSD, unlist(normalPowerT), unlist(normalPowerW))
colnames(dfNormal) <- c("mean", "sd", "power T-test", "power Wilcox-test")

dfCauchy <- data.frame(cauchyLoc, cauchyScale, unlist(cauchyPowerT), unlist(cauchyPowerW))
colnames(dfCauchy) <- c("location", "scale", "power T-test", "power Wilcox-test")

dfUni <- data.frame((uniMax+uniMin)/2, (uniMax-uniMin)/2, unlist(uniPowerT), unlist(uniPowerW))
colnames(dfUni) <- c("mean", "+- interval", "power T-test", "power Wilcox-test")

knitr::kable(dfNormal, caption = "normalverteiltes Rasuchen")
knitr::kable(dfUni, caption = "uniformes Rauschen")
knitr::kable(dfCauchy, caption = "cauchyverteiltes Rauschen")
```

#### Normalverteiltes Rauschen
Wie zu erwarten gibt es beim normalverteilten Rauschen kaum merkliche Unterschiede.


#### Uniformes Rauschen
t-test Überraschend gut im Vergleich zu Normal, quasi identisch. Liegt vermutlich an der ...


#### Cauchy-Rauschen
Deutlich schlechter als Wilcox, Cauchy-Verteilung stärker konzentriert als Normalverteilung, t-Test überschätzt Tails.


# 2. Dichteschätzung

## Kerndichteschätzer

```{r Kernel density estimation}
fnKernelDensityEst <- function(x, X, K, h) {
  # This function computes the kernel density estimates for a given sample X and
  # kernel K with bandwidth h at points x.
  # 
  # Args:
  #   x: Points for which the kernel density estimates are computed
  #   X: Data sample on which the kernel density estimation is fitted
  #   K: Kernel function to be used for smoothing
  #   h: Smoothing bandwidth
  #   
  # Returns:
  #   A vector of the kernel density estimates at points x
  
  # Compute kernel density estimation values using outer
  n <- length(X)
  mKernels <- K((1/h) * outer(X, x, FUN = "-"))
  return((1/(n*h)) * colSums(mKernels))
}
```

```{r Kernel density plot}
fnKernelPlot <- function(x, X, K, vh = NULL, vm = NULL,
                         fnEstimation = fnKernelDensityEst,
                         title = NULL, trueDensity = NULL, ...) {
  # This function plots the kernel density estimates for different bandwidths.
  # If trueDensity is specified, the true density will be plotted as well.
  # 
  # Args:
  #   x:            Points for which the kernel density estimates are computed
  #   X:            Data sample on which the kernel density estimation is fitted
  #   K:            Kernel function to be used for smoothing
  #   vh:           Vector of smoothing bandwidths
  #   vm:           Vector of nearest neigbor positions
  #   fnEstimation: Type of Estimation used
  #   title:        Main title of the plot
  #   trueDensity:  True density to be plotted (e.g. dnorm)
  #   ...:          Further arguments passed to or from other methods
  #   
  # Returns:
  #   -
  
  if(!is.null(vh)) {
    vPar <- vh
    parString <- "h"
  } else if(!is.null(vm)) {
    vPar <-  vm
    parString <- "m"
  } else {
    stop("You need to set the bandwith vh OR the neighbor vm")
  }
  
  nh <- length(vPar)
  
  # Create color palette for number of bandwidths
  cols <- rainbow(nh, alpha = 1)
  
  # Initialize plot with first kernel density
  plot(x, fnEstimation(x, X, K, vPar[1]), type = "l", col = cols[1], 
       main = title, xlab = "x", ylab = "Density", ...)
  
  # Add kernel density for each additional bandwidth
  if (nh > 1) {
    for (i in 2:nh) {
      lines(x, fnEstimation(x, X, K, vPar[i]), col = cols[i])
    }
  }
  
  # Create legend
  legend <- sapply(vPar, function(par) {par <- paste(parString, " = ", round(par, 4))})
  
  # Plot true density if specified
  if (!is.null(trueDensity)) {
    lines(x, trueDensity(x), col = "black")
    legend <- c("True density", legend)
    cols <- c("black", cols)
  }
  
  # Plot legend
  legend("topright", legend = legend, col = cols, lty = 1, cex = 0.75)
}
```

```{r Smoothing Kernels}
# Define different smoothing kernels

fnRectangularKernel <- function(x) {
  return(0.5 * (abs(x) <= 1))
}

fnGaussianKernel <- function(x) {
  return((1/sqrt(2*pi)) * exp((-0.5) * x^2))
}

fnEpanechnikovKernel <- function(x) {
  return(0.75 * (1 - x^2) * (abs(x) <= 1))
}
```

### Anwendung auf standardnormalverteilte Zufallszahlen

```{r Kernel density estimation for standard normal}
# Generate sample
set.seed(42)
n <- 50
X <- rnorm(n)

# Define points to estimate kernel density on
x <- seq(-4, 4, 0.01)

# Set different bandwidths
h <- c(0.1, bw.ucv(X), 1, 4)

# Rectangular kernel density estimation
fnKernelPlot(x, X, fnRectangularKernel, h, 
             title = "Rectangular Kernel Density Estimation",
             trueDensity = dnorm, ylim = c(0, 0.6))

# Gaussian kernel density estimation
fnKernelPlot(x, X, fnGaussianKernel, h, 
             title = "Gaussian Kernel Density Estimation",
             trueDensity = dnorm, ylim = c(0, 0.6))

# Epanechnikov kernel density estimation
fnKernelPlot(x, X, fnEpanechnikovKernel, h, 
             title = "Epanechnikov Kernel Density Estimation",
             trueDensity = dnorm, ylim = c(0, 0.6))
```

### Anwendung auf gleichverteilte Zufallszahlen

```{r Kernel density estimation for uniform}
# Generate sample
set.seed(42)
n <- 50
X <- runif(n)

# Define points to estimate kernel density on
x <- seq(0, 1, 0.01)

# Set different bandwidths
h <- c(bw.ucv(X), 0.1, 0.5)

# Rectangular kernel density estimation
fnKernelPlot(x, X, fnRectangularKernel, h, 
             title = "Rectangular Kernel Density Estimation",
             trueDensity = dunif, ylim = c(0, 2))

# Gaussian kernel density estimation
fnKernelPlot(x, X, fnGaussianKernel, h, 
             title = "Gaussian Kernel Density Estimation",
             trueDensity = dunif, ylim = c(0, 2))

# Epanechnikov kernel density estimation
fnKernelPlot(x, X, fnEpanechnikovKernel, h, 
             title = "Epanechnikov Kernel Density Estimation",
             trueDensity = dunif, ylim = c(0, 2))
```

### Anwendung auf Cauchy-verteilte Zufallszahlen

```{r Kernel density estimation for Cauchy}
# Generate sample
set.seed(42)
n <- 50
X <- rt(n, df = 1)

# Define points to estimate kernel density on
x <- seq(-4, 4, 0.01)

# Set different bandwidths
h <- c(0.5, 1, 5, bw.ucv(X))

# Define Cauchy density function
dcauchy <- function(x) {y <- dt(x, df = 1)}

# Rectangular kernel density estimation
fnKernelPlot(x, X, fnRectangularKernel, h, 
             title = "Rectangular Kernel Density Estimation",
             trueDensity = dcauchy, ylim = c(0, 0.4))

# Gaussian kernel density estimation
fnKernelPlot(x, X, fnGaussianKernel, h, 
             title = "Gaussian Kernel Density Estimation",
             trueDensity = dcauchy, ylim = c(0, 0.4))

# Epanechnikov kernel density estimation
fnKernelPlot(x, X, fnEpanechnikovKernel, h, 
             title = "Epanechnikov Kernel Density Estimation",
             trueDensity = dcauchy, ylim = c(0, 0.4))
```

### Anwendung auf faithful Datensatz

```{r faithful kernel density estimation}
# Faithful kernel density estimation
X <- faithful$eruptions

# Define points to estimate kernel density on
x <- seq(min(X), max(X), 0.01)

# Set different bandwidths
h <- c(0.05, bw.ucv(X), 0.5, 1)

# Gaussian kernel density estimation
title <- "Gaussian Kernel Density Estimation for faithful eruption data"
fnKernelPlot(x, X, fnGaussianKernel, h, title = title)
```


## Kreuzvalidierung zur Bandweitenwahl

<!-- (a) LR: Erwartungstreue habe ich auf Papier bereits gezeigt. -->

```{r Kernel density estimation cross validation}
fnJ <- function(X, K, h) {
  # This function computes the value of the unbiased cross validation criteria.
  # The minimum of this function in h gives an estimated “optimal” (in the
  # sense of mean integrated squared error) bandwidth.
  # 
  # Args:
  #   X: Data sample on which the kernel density estimation is fitted
  #   K: Kernel function to be used for smoothing
  #   h: Smoothing bandwidth
  #   
  # Returns:
  #   Function value
  
  n <- length(X)
  
  # Compute integral of squared kernel density estimator
  integrand <- function(x) {return(fnKernelDensityEst(x, X, K, h)^2)}
  SKDEInt <- integrate(integrand, -Inf, Inf)$value
  
  # Compute G
  mKernels <- K((1/h) * outer(X, X, FUN = "-"))
  diag(mKernels) <- 0
  G <- (1/(n*(n-1)*h)) * sum(mKernels)
  
  # Return function value of J
  return(SKDEInt - 2*G)
}

fnUCV <- function(X, K, hmin, hmax, tol = 0.1 * hmin) {
  # This function returns the minimum of the unbiased cross validation criteria
  # in h which is “optimal” in the sense of mean integrated squared error.
  # We use R's optimize function to find the minimum.
  # 
  # Args:
  #   X:    Data sample on which the kernel density estimation is fitted
  #   K:    Kernel function to be used for smoothing
  #   hmin: Lower bound for optimal h
  #   hmax: Upper bound for optimal h
  #   tol:  The desired accuracy
  #   
  # Returns:
  #   Optimal bandwidth h*
  
  # Find and return the minimum using optimize
  fnTarget <- function(h) {return(fnJ(X, K, h))}
  hOpt <- optimize(fnTarget, c(hmin, hmax), tol = tol)
  return(hOpt$minimum)
}
```

```{r Cross validation test}
hmin <- 0.01
hmax <- 10
fnUCV(faithful$eruptions, fnGaussianKernel, hmin, hmax)
bw.ucv(faithful$eruptions)
```

## m-nächste Nachbarn

```{r m-nearest neighbors}
fnMNearestNeighbors <- function(x, X, K, m) {
  # This function computes the kernel density estimates for a given sample X and
  # kernel K with vector bandwidths h for every point, which is estimated by
  # nearest neighbor approach
  # 
  # Args:
  #   x: Points for which the kernel density estimates are computed
  #   X: Data sample on which the kernel density estimation is fitted
  #   K: Kernel function to be used for smoothing
  #   m: Neighbor position
  #   
  # Returns:
  #   A vector of the nearest neighbor kernel density estimates at points x
  
  n <- length(X)
  
  # Compute bandwidth for every point x
  h <- apply(abs(outer(X, x, "-")), 2, sort)[m,]
  
  # Compute kernel density estimation values using outer
  mKernels <- K((1/t(matrix(rep(h, n), ncol = n))) * outer(X, x, FUN = "-"))
  return((1/(n*h)) * colSums(mKernels))
}

# Generate sample
set.seed(42)
n <- 50
X <- rnorm(n)

# Define points to estimate kernel density on
x <- seq(-4, 4, 0.01)

# Define vector of neighbor positions m
m <- c(5,10,15,25,50)

# Nearest Neighbor Rectangular kernel density estimation
fnKernelPlot(x, X, fnRectangularKernel, vm = m, fnEstimation = fnMNearestNeighbors,
             title = "Nearest Neighbor Rectangular Kernel Density Estimation",
             trueDensity = dnorm, ylim = c(0, 0.6))

# Nearest Neighbor Gaussian kernel density estimation
fnKernelPlot(x, X, fnGaussianKernel, vm = m, fnEstimation = fnMNearestNeighbors,
             title = "Nearest Neighbor Gaussian Kernel Density Estimation",
             trueDensity = dnorm, ylim = c(0, 0.6))

# Nearest Neighbor Epanechnikov kernel density estimation
fnKernelPlot(x, X, fnEpanechnikovKernel, vm = m, fnEstimation = fnMNearestNeighbors, 
             title = "Epanechnikov Kernel Density Estimation",
             trueDensity = dnorm, ylim = c(0, 0.6))
```

# 3. Bildentrauschen

<!-- 4. (a)-(c) LR: Habe ich ebenso auf Papier bereits gezeigt. -->

```{r Load and prepare images}
# Load package
library(EBImage)

# Load image from parent directory
imgLenaColor <- readImage("../lena.png")

# Change image to grayscale
imgLenaGray <- channel(imgLenaColor, "gray")

# Display images
par(mfrow = c(1,2))
display(imgLenaColor, method = "raster")
display(imgLenaGray, method = "raster")
```

```{r fnAddNoise}
fnAddNoise <- function(img, rnoise, ...) {
  # This function adds noise specified by rnoise to a grayscale image. If the
  # image is not grayscale, it gets converted to grayscale first.
  # 
  # Args:
  #   img:    Image
  #   rnoise: Random noise generation function (e.g. rnorm for Gaussian noise)
  #   ...:    Further arguments passed to or from other methods
  #   
  # Returns:
  #   Grayscale image with added noise
  
  # Check if grayscale and convert if necessary
  if(colorMode(img) != 0) {img <- channel(img, "gray")}
  
  # Add noise
  m <- dim(img)[1]
  p <- dim(img)[2]
  imgNoise <- Image(imageData(img) + matrix(rnoise(m*p, ...), m, p))
  
  # Adjust values below 0 and above 1
  imgNoise[imgNoise < 0] <- 0
  imgNoise[imgNoise > 1] <- 1
  
  return(imgNoise)
}
```

```{r Noise Example}
imgLenaNoise1 <- fnAddNoise(imgLenaGray, rnorm, sd = 0.1)
imgLenaNoise2 <- fnAddNoise(imgLenaGray, rnorm, sd = 0.25)
imgLenaNoise3 <- fnAddNoise(imgLenaGray, rnorm, sd = 0.5)

par(mfrow = c(1,3))
display(imgLenaNoise1, method = "raster")
display(imgLenaNoise2, method = "raster")
display(imgLenaNoise3, method = "raster")
```

```{r fnNadarayaWatson}
fnNadarayaWatson <- function(img, K, h) {
  # The Nadaraya-Watson-estimator with weights defined by kernel K and bandwidth
  # h for denoising/smoothing an image
  # 
  # Args:
  #   img:  Image to be denoised
  #   K:    Smoothing kernel used in weights
  #   h:    Bandwidth used in weights
  #   
  # Returns:
  #   Denoised/smoothed image
  
  # Check if grayscale and convert if necessary
  if(colorMode(img) != 0) {img <- channel(img, "gray")}
  
  # Get Y and dimensions
  Y <- imageData(img)
  m <- dim(img)[1]
  p <- dim(img)[2]
  
  # Generate M and N
  M <- K((1/h) * outer(1:m, 1:m, FUN = "-"))
  M <- M / rowSums(M)
  
  N <- K((1/h) * outer(1:p, 1:p, FUN = "-"))
  N <- N / rowSums(N)
  
  return(Image(M %*% Y %*% t(N)))
}
```

```{r Nadaraya-Watson-estimator evaluation}
fnEvalNWGaussianNoise <- function(img, vsigma, vh) {
  # This function is a wrapper to compare the denoising results of images with 
  # different levels of Gaussian noise for the Nadaraya-Watson-estimator with
  # weights defined by the Gaussian kernel and rectangular kernel respectively 
  # for different bandwidths h.
  # 
  # Args:
  #   img:    Image
  #   sigma:  Vector of standard deviations used to add noise
  #   h:      Vector of bandwidths used in weights of the
  #           Nadaraya-Watson-estimator
  #   
  # Returns:
  #   -
  
  # Define helper function for label implementation
  fnLabel <- function(label) {
    text(x = 20, y = 20, adj = c(0,1), col = "red", cex = 1.5, label = label)
  }
  
  # Set size of frame
  par(mfrow = c(1, length(vsigma)))
  
  # Initalize noise image list
  imagesNoise <- list()
  
  for (i in 1:length(vsigma)) {
    # Add noise to image and plot
    imgNoise <- fnAddNoise(img, rnorm, sd = vsigma[i])
    imagesNoise[i] <- list(imgNoise)
    display(imgNoise, method = "raster")
    fnLabel(substitute(paste(sigma, "=", sd), list(sd=vsigma[i])))
  }
  
  # Set size of frame for plots below
  par(mfrow = c(length(vh), length(vsigma)))
  
  cat('Gaussian kernel')
  for (h in vh) {
    # NW-Denoising with Gaussian kernel
    for (i in 1:length(vsigma)) {
      display(fnNadarayaWatson(imagesNoise[[i]], fnGaussianKernel, h),
              method = "raster")
      fnLabel(substitute(paste(sigma, "=", sd, ", h=", h),
                         list(sd=vsigma[i], h=h)))
    }
  }
  
  cat('Rectangular kernel')
  for (h in vh) { 
    # NW-Denoising with rectangular kernel
    for (i in 1:length(vsigma)) {
      display(fnNadarayaWatson(imagesNoise[[i]], fnRectangularKernel, h), 
              method = "raster")
      fnLabel(substitute(paste(sigma, "=", sd, ", h=", h),
                         list(sd=vsigma[i], h=h)))
    }
  }
}
```

```{r Nadaraya-Watson-estimator Test}
sigma <- c(0.1, 0.25, 0.5)
h <- c(0.1, 1, 10)

# Lena
fnEvalNWGaussianNoise(imgLenaGray, sigma, h)

# Porsche
imgPorsche <- readImage("../porsche.jpeg")
fnEvalNWGaussianNoise(imgPorsche, sigma, h)
```

```{r Weighted Median}
# 7. The weighted median is more robust than the mean
```

## Weitere Methoden zur Bildentrauschung

### Rangordnungsfilter

Bei Rangordnungsfilter wird eine bestimmte Anzahl von Grauwerten in einer Umgebung eines Pixels betrachtet. Die so erfassten Grauwerte werden dem Rang nach in einer Liste sortiert, also nach Größe des Grauwertes. Der aktuell betrachtete Pixel wird durch einen Grauwert aus der Liste ersetzt. Dabei kann für die Wahl der Position ein beliebiges Verfahren eingesetzt werden, z.B. Minimumfilter (minimaler Wert aus der Liste), Maximumfilter (maximaler Wert aus der Liste), Medianfilter (der Grauwert in der Mitte der Liste), etc.

Rangordnungsfilter eignen sich für Bilder mit Ausreißern, also z.B. Kratzer oder einzelne deutlich abweichende Pixel.

### Frequenzraumfilter

Ein Bild kann sowohl im Ortsraum, als auch im Freuqenzraum beschrieben werden. Zur Transformation aus dem Ortsraum in den Frequenzraum wird eine diskrete Fouriertransformation durchgeführt. Anschließend können verschiedene Filter verwendet werden, z.B. Hochpass- oder Tiefpassfilter. Zufälliges Rauschen kann als hochfrequent angenommen werden, weshalb sich hier ein Tiefpassfilter eignen würde (die niedrigen Frequenzen bleiben unverändert). Dabei kann der Filter "hart" abschneiden oder einen Übergangsbereich definieren. Nach der Filterung werden die Daten wieder in den Ortsraum zurück transformiert.

Anwednungsfälle für den Frequenzraumfilter ergeben sich je nach Fragestellung. Für verschiedene Fälle eignen sich entsprechende Filter im Frequenzraum.


<!--Footnotes-->
